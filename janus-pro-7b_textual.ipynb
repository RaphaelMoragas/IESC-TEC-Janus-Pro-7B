{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64adb66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael.moragas/proceGPT/proceGPT-Janus/janus-pro-7b/VENV_janusLLM/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:594: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some kwargs in processor config are unused and will not have any effect: num_image_tokens, add_special_token, mask_prompt, ignore_id, sft_format, image_tag. \n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8808b8a3b9bc4d319154960e1d6ecb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"deepseek-ai/Janus-Pro-7B\"\n",
    "\n",
    "# Carrega processor e modelo multimodal\n",
    "processor = VLChatProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = MultiModalityCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# Garante template de chat padrão\n",
    "if getattr(processor, \"chat_template\", None) is None:\n",
    "    processor.chat_template = [\n",
    "        {\"role\":\"system\",\"content\":\"You are a helpful assistant that can read document pages and answer questions based on them.\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73632f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path: str, output_folder: str) -> None:\n",
    "    output_dir = Path(output_folder)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    for i, page in enumerate(pdf, start=1):\n",
    "        out_path = output_dir / f\"page_{i}.png\"\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))\n",
    "        img = Image.open(pix.tobytes(\"png\")).convert(\"RGB\")\n",
    "        img = img.resize((384,384), resample=Image.LANCZOS)\n",
    "        img.save(out_path, format=\"PNG\")\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ae3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(folder: str) -> list[str]:\n",
    "\n",
    "    pngs = [p for p in Path(folder).iterdir() if p.suffix.lower() == \".png\"]\n",
    "    pngs.sort(key=lambda p: int(p.stem.split(\"_\")[1]))\n",
    "    return [str(p) for p in pngs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb5a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resposta_textual(image_path: str, question: str, max_new_tokens=100) -> str:\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    conversation = [\n",
    "        {\"role\":\"user\",\"content\":f\"<image_placeholder>\\n{question}\"}\n",
    "    ]\n",
    "    processed = processor(\n",
    "        conversations=conversation,\n",
    "        images=[img],\n",
    "        force_batchify=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device, dtype=torch.float16)\n",
    "    \n",
    "    inputs_embeds = model.prepare_inputs_embeds(\n",
    "        input_ids=processed.input_ids,\n",
    "        pixel_values=processed.pixel_values,\n",
    "        images_seq_mask=processed.images_seq_mask,\n",
    "        images_emb_mask=processed.images_emb_mask\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated = model.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=processed.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False\n",
    "        )\n",
    "    return tokenizer.batch_decode(generated, skip_special_tokens=True)[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab251ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta para page_106.png:\n",
      "Este é um exemplo de uma rede de trabalho em que os trabalhadores compartilham informaç�es e recursos.Este é um exemplo de uma rede de trabalho em que os trabalhadores compartilham informaç�es e recursos.\n",
      "Este é um exemplo de uma rede de trabalho em que os trabalhadores compartilham informaç�es e recursos.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_to_images(\"JCRIBEIRO.R02.TOBE.V5.92.pdf\", \"Documents_Image_Janus\")\n",
    "paths = get_image_paths(\"Documents_Image_Janus\")\n",
    "\n",
    "# modificar para a página que vc quer a resposta da pergunta feita abaixo\n",
    "page_number = 106\n",
    "\n",
    "image_path = paths[page_number - 1]  \n",
    "\n",
    "# definir a pergunta que vc quer fazer da página (de preferencia em inglês)\n",
    "pergunta = \"Pode me dizer a principal ideia desta pagina?\"\n",
    "\n",
    "resposta = resposta_textual(image_path, pergunta)\n",
    "print(f\"Resposta para {Path(image_path).name}:\\n{resposta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV_janusLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
